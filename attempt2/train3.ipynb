{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keLDMa7DXWSD",
        "outputId": "f82152b5-6e3b-4ff0-ba2c-be8dbcd9830a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "%pip install datasets\n",
        "%pip install peft\n",
        "%pip install -U bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a3yRIa4jXWSF"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import psutil\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "shqSQr85XWSG"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def format_example(example):\n",
        "    \"\"\"Format a single example for fine-tuning.\"\"\"\n",
        "    return f\"\"\"Problem: {example['markdown_description']}\\n\\nSolution: {example['canonical_solution']}\"\"\"\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xGNVCVvTXWSG"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def prepare_dataset():\n",
        "    \"\"\"Prepare the Effibench dataset for fine-tuning.\"\"\"\n",
        "    dataset = load_dataset(\"DONG19/EffiBench\")\n",
        "    print(\"Sample record:\", dataset['train'][0])  # Inspect a single record\n",
        "\n",
        "    def format_function(examples):\n",
        "        inputs = [f\"Problem: {md}\" for md in examples[\"markdown_description\"]]\n",
        "        outputs = [f\"Solution: {cs}\" for cs in examples[\"canonical_solution\"]]\n",
        "        return {\"input_text\": inputs, \"output_text\": outputs}\n",
        "\n",
        "    # Tokenization function\n",
        "    def tokenize_function(examples):\n",
        "        inputs = tokenizer(\n",
        "            examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=1024\n",
        "        )\n",
        "        outputs = tokenizer(\n",
        "            examples[\"output_text\"], padding=\"max_length\", truncation=True, max_length=1024\n",
        "        )\n",
        "        inputs[\"labels\"] = outputs[\"input_ids\"]\n",
        "        return inputs\n",
        "\n",
        "    # Format the dataset\n",
        "    formatted_dataset = dataset.map(\n",
        "        format_function,\n",
        "        remove_columns=dataset[\"train\"].column_names,\n",
        "        batched=True  # Process in batches\n",
        "    )\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    tokenized_dataset = formatted_dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "    return tokenized_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TCI7CPJNXWSH"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Initialize memory tracking\n",
        "initial_memory = get_memory_usage()\n",
        "memory_measurements = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "453c99857b114aeb92810aa363cd6157",
            "0590d2626c554b2aa544836c1733d806",
            "e44061dd4d98465fa13640bac8a5ba80",
            "8db651c10a6f43c781adbc75957ca309",
            "895f00d72a634d499b410ccb7899851d",
            "b1045b95fd594ef7a2128b1305d8f1a5",
            "2daddbfe088845cd8a2afdb7fffc8d6a",
            "fad18533ae3d475b8e3333092bda6aef",
            "701847283be54cd0a089f714ed0fdc4d",
            "e9f16967ecd749f0b9b3dc283898bb03",
            "9f08bbb0fbc0432dbeef5e05c2ffba1e"
          ]
        },
        "id": "UmOX8GkhXWSH",
        "outputId": "bf2b2006-0f68-47be-e8ec-92d61b627cec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "453c99857b114aeb92810aa363cd6157"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# %%\n",
        "# Model configuration\n",
        "model_name = \"NousResearch/Llama-2-7b-hf\"  # Using 7B parameter model as it's more consumer-friendly\n",
        "\n",
        "# QLoRA configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DGXbV5lsXWSI"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ2SEQ_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "e6087dba279c4c8db8964953cd197a3c",
            "b521695202c54da39825c955fdfaa0f0",
            "1dfe1099f2d34f3c9dee5e0e0ba9342b",
            "912f789b8a324bb2a2e3c077d383d235",
            "c2eb6273b4fe4ce5bb0601aa16d9a628",
            "e4532a5392814e88840a4aa4ba36e078",
            "7eab3d1123f546468d488e1996079ae4",
            "91c3720c3a0049788d24d8ecce1acff1",
            "380df8e24d52484f9697e1f512f4f654",
            "969d86e767d94f139d800dcc69ea7952",
            "98887f98d2a24d7fbdfc4c48ea6a9d33"
          ]
        },
        "id": "NH1dF3t6XWSI",
        "outputId": "ad63ee60-596a-4425-85fd-63362c6b2dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample record: {'problem_idx': 3, 'task_name': 'Longest Substring Without Repeating Characters', 'description': '\\n\\n<p>Given a string <code>s</code>, find the length of the <strong>longest</strong> <span data-keyword=\"substring-nonempty\"><strong>substring</strong></span> without repeating characters.</p>\\n\\n<p>&nbsp;</p>\\n<p><strong class=\"example\">Example 1:</strong></p>\\n\\n<pre>\\n<strong>Input:</strong> s = &quot;abcabcbb&quot;\\n<strong>Output:</strong> 3\\n<strong>Explanation:</strong> The answer is &quot;abc&quot;, with the length of 3.\\n</pre>\\n\\n<p><strong class=\"example\">Example 2:</strong></p>\\n\\n<pre>\\n<strong>Input:</strong> s = &quot;bbbbb&quot;\\n<strong>Output:</strong> 1\\n<strong>Explanation:</strong> The answer is &quot;b&quot;, with the length of 1.\\n</pre>\\n\\n<p><strong class=\"example\">Example 3:</strong></p>\\n\\n<pre>\\n<strong>Input:</strong> s = &quot;pwwkew&quot;\\n<strong>Output:</strong> 3\\n<strong>Explanation:</strong> The answer is &quot;wke&quot;, with the length of 3.\\nNotice that the answer must be a substring, &quot;pwke&quot; is a subsequence and not a substring.\\n</pre>\\n\\n<p>&nbsp;</p>\\n<p><strong>Constraints:</strong></p>\\n\\n<ul>\\n\\t<li><code>0 &lt;= s.length &lt;= 5 * 10<sup>4</sup></code></li>\\n\\t<li><code>s</code> consists of English letters, digits, symbols and spaces.</li>\\n</ul>\\n\\n', 'markdown_description': '\\nGiven a string `s`, find the length of the **longest** **substring** without repeating characters.\\n\\n\\n\\xa0\\n\\n\\n**Example 1:**\\n\\n\\n\\n```\\n\\n**Input:** s = \"abcabcbb\"\\n**Output:** 3\\n**Explanation:** The answer is \"abc\", with the length of 3.\\n\\n```\\n\\n**Example 2:**\\n\\n\\n\\n```\\n\\n**Input:** s = \"bbbbb\"\\n**Output:** 1\\n**Explanation:** The answer is \"b\", with the length of 1.\\n\\n```\\n\\n**Example 3:**\\n\\n\\n\\n```\\n\\n**Input:** s = \"pwwkew\"\\n**Output:** 3\\n**Explanation:** The answer is \"wke\", with the length of 3.\\nNotice that the answer must be a substring, \"pwke\" is a subsequence and not a substring.\\n\\n```\\n\\n\\xa0\\n\\n\\n**Constraints:**\\n\\n\\n* `0 <= s.length <= 5 * 104`\\n* `s` consists of English letters, digits, symbols and spaces.\\n\\n\\n', 'canonical_solution': 'class Solution:\\n    def lengthOfLongestSubstring(self, s: str) -> int:\\n        ss = set()\\n        i = ans = 0\\n        for j, c in enumerate(s):\\n            while c in ss:\\n                ss.remove(s[i])\\n                i += 1\\n            ss.add(c)\\n            ans = max(ans, j - i + 1)\\n        return ans\\n', 'test_case_generator': '\\nimport random\\n\\nclass Solution:\\n    def lengthOfLongestSubstring(self, s: str) -> int:\\n        ss = set()\\n        i = ans = 0\\n        for j, c in enumerate(s):\\n            while c in ss:\\n                ss.remove(s[i])\\n                i += 1\\n            ss.add(c)\\n            ans = max(ans, j - i + 1)\\n        return ans\\n\\ndef generate_test_case():\\n    solution = Solution()\\n\\n    # Generate a random string\\n    s = \\'\\'.join(random.choices(\\'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\\', k=random.randint(0, 10)))\\n\\n    # Calculate the expected result using the provided Solution class\\n    expected_result = solution.lengthOfLongestSubstring(s)\\n\\n    return (s, ), expected_result\\n\\ndef test_generated_test_cases(num_tests):\\n    test_case_generator_results = []\\n    for i in range(num_tests):\\n        inputs, expected_result = generate_test_case()\\n        solution = Solution()\\n        assert solution.lengthOfLongestSubstring(*inputs) == expected_result\\n\\n        test_case_generator_results.append(f\"assert solution.lengthOfLongestSubstring({\\', \\'.join(map(repr, inputs))}) == {expected_result}\")\\n    return test_case_generator_results\\n\\nif __name__ == \\'__main__\\':\\n    num_tests = 100\\n    test_case_generator_results = test_generated_test_cases(num_tests)\\n    print(*test_case_generator_results, sep=\"\\\\n\")\\n', 'test_case': \"assert solution.lengthOfLongestSubstring('') == 0\\nassert solution.lengthOfLongestSubstring('krLKl6F') == 7\\nassert solution.lengthOfLongestSubstring('p2Cn3Y6') == 7\\nassert solution.lengthOfLongestSubstring('jf') == 2\\nassert solution.lengthOfLongestSubstring('ebl') == 3\\nassert solution.lengthOfLongestSubstring('7FHbLe') == 6\\nassert solution.lengthOfLongestSubstring('cUoD0S') == 6\\nassert solution.lengthOfLongestSubstring('M1kCixrcvS') == 10\\nassert solution.lengthOfLongestSubstring('V9sGI') == 5\\nassert solution.lengthOfLongestSubstring('0iTSFPsD5') == 9\\nassert solution.lengthOfLongestSubstring('D') == 1\\nassert solution.lengthOfLongestSubstring('ncNp') == 4\\nassert solution.lengthOfLongestSubstring('FKdZH') == 5\\nassert solution.lengthOfLongestSubstring('FqhvT67') == 7\\nassert solution.lengthOfLongestSubstring('D1zyWG0a') == 8\\nassert solution.lengthOfLongestSubstring('VyOucQ') == 6\\nassert solution.lengthOfLongestSubstring('SXqMDBVoEN') == 10\\nassert solution.lengthOfLongestSubstring('mYjdE1QjDm') == 7\\nassert solution.lengthOfLongestSubstring('4rd2vxkZZR') == 8\\nassert solution.lengthOfLongestSubstring('qWggGpiX') == 5\\nassert solution.lengthOfLongestSubstring('yEk') == 3\\nassert solution.lengthOfLongestSubstring('1SbbkRXgx') == 6\\nassert solution.lengthOfLongestSubstring('WnLZ') == 4\\nassert solution.lengthOfLongestSubstring('pEF') == 3\\nassert solution.lengthOfLongestSubstring('d7OCVynX0') == 9\\nassert solution.lengthOfLongestSubstring('') == 0\\nassert solution.lengthOfLongestSubstring('qSPMKL2Pa') == 7\\nassert solution.lengthOfLongestSubstring('PrYzK1nUJ') == 9\\nassert solution.lengthOfLongestSubstring('JBu') == 3\\nassert solution.lengthOfLongestSubstring('IBCg7KcjgY') == 8\\nassert solution.lengthOfLongestSubstring('T') == 1\\nassert solution.lengthOfLongestSubstring('Rjy5bwtsK') == 9\\nassert solution.lengthOfLongestSubstring('Brh') == 3\\nassert solution.lengthOfLongestSubstring('DPz') == 3\\nassert solution.lengthOfLongestSubstring('kSu99Brx') == 4\\nassert solution.lengthOfLongestSubstring('m') == 1\\nassert solution.lengthOfLongestSubstring('KMa') == 3\\nassert solution.lengthOfLongestSubstring('VLOg') == 4\\nassert solution.lengthOfLongestSubstring('UmRlK') == 5\\nassert solution.lengthOfLongestSubstring('AWw2zq') == 6\\nassert solution.lengthOfLongestSubstring('VBs2uAH3H') == 8\\nassert solution.lengthOfLongestSubstring('ufSovl54BW') == 10\\nassert solution.lengthOfLongestSubstring('vStUY7') == 6\\nassert solution.lengthOfLongestSubstring('oGF') == 3\\nassert solution.lengthOfLongestSubstring('0Xj') == 3\\nassert solution.lengthOfLongestSubstring('4') == 1\\nassert solution.lengthOfLongestSubstring('aFevv') == 4\\nassert solution.lengthOfLongestSubstring('Xz') == 2\\nassert solution.lengthOfLongestSubstring('StG4') == 4\\nassert solution.lengthOfLongestSubstring('Rie') == 3\\nassert solution.lengthOfLongestSubstring('8RY3erTER') == 8\\nassert solution.lengthOfLongestSubstring('BN2n6AoJ0c') == 10\\nassert solution.lengthOfLongestSubstring('UO') == 2\\nassert solution.lengthOfLongestSubstring('VmBZvv1') == 5\\nassert solution.lengthOfLongestSubstring('LMWQ') == 4\\nassert solution.lengthOfLongestSubstring('') == 0\\nassert solution.lengthOfLongestSubstring('80') == 2\\nassert solution.lengthOfLongestSubstring('xvr') == 3\\nassert solution.lengthOfLongestSubstring('L1sr9dvT') == 8\\nassert solution.lengthOfLongestSubstring('F') == 1\\nassert solution.lengthOfLongestSubstring('9mBKJg2RoF') == 10\\nassert solution.lengthOfLongestSubstring('u3SHz53') == 6\\nassert solution.lengthOfLongestSubstring('uIxsZwqW2u') == 9\\nassert solution.lengthOfLongestSubstring('iiJ30w') == 5\\nassert solution.lengthOfLongestSubstring('LPK5N') == 5\\nassert solution.lengthOfLongestSubstring('MvYNcL') == 6\\nassert solution.lengthOfLongestSubstring('PidX') == 4\\nassert solution.lengthOfLongestSubstring('c0c') == 2\\nassert solution.lengthOfLongestSubstring('') == 0\\nassert solution.lengthOfLongestSubstring('UUBXog7At') == 8\\nassert solution.lengthOfLongestSubstring('uY9RR0') == 4\\nassert solution.lengthOfLongestSubstring('') == 0\\nassert solution.lengthOfLongestSubstring('SLWSu53h') == 7\\nassert solution.lengthOfLongestSubstring('vhsRBcC') == 7\\nassert solution.lengthOfLongestSubstring('WDkR2G3jSH') == 10\\nassert solution.lengthOfLongestSubstring('') == 0\\nassert solution.lengthOfLongestSubstring('Ssid5') == 5\\nassert solution.lengthOfLongestSubstring('xLZ8IF1FIT') == 7\\nassert solution.lengthOfLongestSubstring('9day') == 4\\nassert solution.lengthOfLongestSubstring('uPvh71l') == 7\\nassert solution.lengthOfLongestSubstring('AICk') == 4\\nassert solution.lengthOfLongestSubstring('E5D') == 3\\nassert solution.lengthOfLongestSubstring('eJ5D1gPo') == 8\\nassert solution.lengthOfLongestSubstring('OdrwR') == 5\\nassert solution.lengthOfLongestSubstring('ztJjPukfhk') == 9\\nassert solution.lengthOfLongestSubstring('gE') == 2\\nassert solution.lengthOfLongestSubstring('Yp0') == 3\\nassert solution.lengthOfLongestSubstring('Ue9Pqe74k') == 7\\nassert solution.lengthOfLongestSubstring('5') == 1\\nassert solution.lengthOfLongestSubstring('BdauxZZFxu') == 6\\nassert solution.lengthOfLongestSubstring('FZhAzP20n') == 9\\nassert solution.lengthOfLongestSubstring('iBFKT') == 5\\nassert solution.lengthOfLongestSubstring('kd') == 2\\nassert solution.lengthOfLongestSubstring('GzLCgm1lF') == 9\\nassert solution.lengthOfLongestSubstring('GxkMbFuM') == 7\\nassert solution.lengthOfLongestSubstring('') == 0\\nassert solution.lengthOfLongestSubstring('OGa7brYx3e') == 10\\nassert solution.lengthOfLongestSubstring('743lBZ2upV') == 10\\nassert solution.lengthOfLongestSubstring('vVj') == 3\\nassert solution.lengthOfLongestSubstring('') == 0\"}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6087dba279c4c8db8964953cd197a3c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# %%\n",
        "# Prepare dataset\n",
        "dataset = prepare_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K5CPDW1pXWSI"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    # predict_with_generate=True  # Required for seq2seq tasks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gvJlsGuTXWSI"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Data collator for seq2seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WhGjdPYBXWSJ"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Record pre-training memory\n",
        "pre_training_memory = get_memory_usage()\n",
        "memory_measurements.append((\"Pre-training\", pre_training_memory))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fM58LjvZXWSJ",
        "outputId": "0bfc7c3d-9682-4f3c-c9f3-e1dc1d766875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marnavjoshi007\u001b[0m (\u001b[33marnavjoshi007-northeastern-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241208_232233-hykwf082</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arnavjoshi007-northeastern-university/huggingface/runs/hykwf082' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/arnavjoshi007-northeastern-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arnavjoshi007-northeastern-university/huggingface' target=\"_blank\">https://wandb.ai/arnavjoshi007-northeastern-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arnavjoshi007-northeastern-university/huggingface/runs/hykwf082' target=\"_blank\">https://wandb.ai/arnavjoshi007-northeastern-university/huggingface/runs/hykwf082</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [62/62 49:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=62, training_loss=5.1577911376953125, metrics={'train_runtime': 2990.905, 'train_samples_per_second': 0.334, 'train_steps_per_second': 0.021, 'total_flos': 4.032188231398195e+16, 'train_loss': 5.1577911376953125, 'epoch': 0.992})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# %%\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxA6ikXPXWSJ",
        "outputId": "d578568d-1d5b-4090-d983-9923bbf08a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Memory Usage Statistics:\n",
            "Initial Memory Usage: 1040.91 MB\n",
            "Pre-training Memory Usage: 1541.10 MB\n",
            "Post-training Memory Usage: 1869.52 MB\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Record post-training memory\n",
        "post_training_memory = get_memory_usage()\n",
        "memory_measurements.append((\"Post-training\", post_training_memory))\n",
        "\n",
        "# Print memory usage statistics\n",
        "print(\"\\nMemory Usage Statistics:\")\n",
        "print(f\"Initial Memory Usage: {initial_memory:.2f} MB\")\n",
        "for stage, memory in memory_measurements:\n",
        "    print(f\"{stage} Memory Usage: {memory:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "model_save_path = \"./fine_tuned_model\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "yrSZwLsHxY__",
        "outputId": "68075d80-a11f-4652-e383-680e2898eb76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to ./fine_tuned_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BxUP1aHMxf4u",
        "outputId": "2f91033d-fc9b-4269-ba86-d30ea67d49a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy model and tokenizer from saved folder to Google Drive\n",
        "!cp -r \"./fine_tuned_model\" \"/content/drive/MyDrive/CS 6158/CS6158 Research Project/fine_tuned_model\""
      ],
      "metadata": {
        "id": "ok6Y-CDgx6AQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_inference(memory_model, memory_tokenizer, test_dataset, num_samples=5):\n",
        "    \"\"\"Perform inference using the model in memory.\"\"\"\n",
        "    # Select a few examples from the test set\n",
        "    test_samples = test_dataset[\"test\"].select(range(num_samples))\n",
        "\n",
        "    for idx, example in enumerate(test_samples):\n",
        "        input_text = f\"Problem: {example['markdown_description']}\"\n",
        "        # Tokenize the input\n",
        "        inputs = memory_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256).to(memory_model.device)\n",
        "\n",
        "        # Generate prediction\n",
        "        outputs = memory_model.generate(**inputs, max_length=256, num_beams=5, early_stopping=True)\n",
        "        prediction = memory_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Display result\n",
        "        print(f\"Test Sample {idx + 1}:\")\n",
        "        print(f\"Input: {input_text}\")\n",
        "        print(f\"Generated Solution: {prediction}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Perform inference using the model in memory\n",
        "perform_inference(model, tokenizer, dataset)"
      ],
      "metadata": {
        "id": "CC0wlAAKyZAu",
        "outputId": "8853d9b7-cf82-4f97-f0e2-97d1d2e30f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'test'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a4024f18ff43>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Perform inference using the model in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mperform_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-a4024f18ff43>\u001b[0m in \u001b[0;36mperform_inference\u001b[0;34m(memory_model, memory_tokenizer, test_dataset, num_samples)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Perform inference using the model in memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Select a few examples from the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtest_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedSplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             available_suggested_splits = [\n",
            "\u001b[0;31mKeyError\u001b[0m: 'test'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nMvqzCxyyiW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "453c99857b114aeb92810aa363cd6157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0590d2626c554b2aa544836c1733d806",
              "IPY_MODEL_e44061dd4d98465fa13640bac8a5ba80",
              "IPY_MODEL_8db651c10a6f43c781adbc75957ca309"
            ],
            "layout": "IPY_MODEL_895f00d72a634d499b410ccb7899851d"
          }
        },
        "0590d2626c554b2aa544836c1733d806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1045b95fd594ef7a2128b1305d8f1a5",
            "placeholder": "​",
            "style": "IPY_MODEL_2daddbfe088845cd8a2afdb7fffc8d6a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e44061dd4d98465fa13640bac8a5ba80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fad18533ae3d475b8e3333092bda6aef",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_701847283be54cd0a089f714ed0fdc4d",
            "value": 2
          }
        },
        "8db651c10a6f43c781adbc75957ca309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9f16967ecd749f0b9b3dc283898bb03",
            "placeholder": "​",
            "style": "IPY_MODEL_9f08bbb0fbc0432dbeef5e05c2ffba1e",
            "value": " 2/2 [01:11&lt;00:00, 32.82s/it]"
          }
        },
        "895f00d72a634d499b410ccb7899851d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1045b95fd594ef7a2128b1305d8f1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2daddbfe088845cd8a2afdb7fffc8d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fad18533ae3d475b8e3333092bda6aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "701847283be54cd0a089f714ed0fdc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9f16967ecd749f0b9b3dc283898bb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f08bbb0fbc0432dbeef5e05c2ffba1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6087dba279c4c8db8964953cd197a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b521695202c54da39825c955fdfaa0f0",
              "IPY_MODEL_1dfe1099f2d34f3c9dee5e0e0ba9342b",
              "IPY_MODEL_912f789b8a324bb2a2e3c077d383d235"
            ],
            "layout": "IPY_MODEL_c2eb6273b4fe4ce5bb0601aa16d9a628"
          }
        },
        "b521695202c54da39825c955fdfaa0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4532a5392814e88840a4aa4ba36e078",
            "placeholder": "​",
            "style": "IPY_MODEL_7eab3d1123f546468d488e1996079ae4",
            "value": "Map: 100%"
          }
        },
        "1dfe1099f2d34f3c9dee5e0e0ba9342b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c3720c3a0049788d24d8ecce1acff1",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_380df8e24d52484f9697e1f512f4f654",
            "value": 1000
          }
        },
        "912f789b8a324bb2a2e3c077d383d235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969d86e767d94f139d800dcc69ea7952",
            "placeholder": "​",
            "style": "IPY_MODEL_98887f98d2a24d7fbdfc4c48ea6a9d33",
            "value": " 1000/1000 [00:01&lt;00:00, 646.39 examples/s]"
          }
        },
        "c2eb6273b4fe4ce5bb0601aa16d9a628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4532a5392814e88840a4aa4ba36e078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eab3d1123f546468d488e1996079ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91c3720c3a0049788d24d8ecce1acff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "380df8e24d52484f9697e1f512f4f654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "969d86e767d94f139d800dcc69ea7952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98887f98d2a24d7fbdfc4c48ea6a9d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}